---
title: "bmes547 hw7"
author: "Jeremy Leipzig"
date: "3/8/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Classification of Active Sites using k-nearest neighbors
In this assignment, you are going to predict catalytic residues in proteins using sequence and structural information. The dataset (courtesy of Natalia Petrova) is a subset of the data used in "Prediction of catalytic residues using Support Vector Machine with selected protein sequence and structural properties", Natalia Petrova and Cathy Wu, 2006.

http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-7-312

Use leave-one-out cross-validation. The number of nearest neighbors are left up to you. You must normalize the data so that each feature has a range between 0 and 1.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
library(nnet)
library(caret)
library(dplyr)
library(stringr)
library(ggplot2)
library(class)
set.seed(2345)
```

```{r, echo=TRUE, warning=FALSE, message=FALSE, error=FALSE, results='hide'}
## Load the data
# Get the data using the loadcatsite() fnction.
source("loadcatsite.R")
X<-loadcatsite()$X
T<-loadcatsite()$T

#normalize all columns
normalizeme<-function(x){(x-min(x))/(max(x)-min(x))}

#1 for rows, 2 for columns
X_norm<-apply(X,2,normalizeme)

test.cl <- function(true, pred) {
  #return confusion matrix
  truef <- factor(colnames(true)[max.col(true)],levels=c(-1,1))
  predf <- factor(colnames(pred)[max.col(pred)],levels=c(-1,1))
  ct<-table(truef, predf)
  return(ct)
}

get_accuracy<-function(resulttable){
  #assumes NEG POS columns and rows
  #Classification accuracy
  #(TP + TN) / (TP + TN + FP + FN)
  TN<-resulttable[1,1]
  TP<-resulttable[2,2]
  FN<-resulttable[2,1]
  FP<-resulttable[1,2]
  accuracy <- ((TP + TN) / (TP + TN + FP + FN))
  return(accuracy)
}

max.nn<-15
lvresults<-list()

training_classification_accuracy<-NULL
test_classification_accuracy<-NULL

train_calc<-list()
test_results<-list()
train_results<-list()

for(nn in 1:max.nn){
  print(paste(nn,"nearestneighbors"))
  lvresults[[nn]]<-list()
  for(lv1out in 1:nrow(X_norm)){
    trainingset <- X_norm[-lv1out,]
    trueclass<- as.factor(T[-lv1out])
    testset <- X_norm[lv1out,]
    lvresults[[nn]][[lv1out]]<-knn(trainingset,testset,trueclass,k=nn)
  }
  #training calc is all data
  train_calc[[nn]]<-knn(X_norm,X_norm,as.factor(T),k=nn)
  train_results[[nn]]<-test.cl(class.ind(T), class.ind(unlist(train_calc[[nn]])))
  training_classification_accuracy[nn]<-get_accuracy(train_results[[nn]])
  
  test_results[[nn]]<-test.cl(class.ind(T), class.ind(unlist(lvresults[[nn]])))
  test_classification_accuracy[nn]<-get_accuracy(test_results[[nn]])
}
data.frame(nearestneighbors=1:max.nn,
           training_classification_accuracy,
           test_classification_accuracy) -> results
```

## What is the training and test accuracies?
For training accuracy I simply used the full set as both testing and training.

```{r, echo=TRUE, warning=FALSE, message=FALSE, error=FALSE}
results %>% melt(id.vars="nearestneighbors") -> melt_results

ggplot(melt_results,aes(nearestneighbors,value))+
  geom_smooth(aes(color=variable))+
  geom_point(aes(color=variable))+
  xlab("k nearestneighbors")+
  ylab("classification accuracy")

knitr::kable(results %>% select(nearestneighbors,test_classification_accuracy))
```

## What is the best number of nearest neighbors?
`r results$nearestneighbors[which(results$test_classification_accuracy==max(results$test_classification_accuracy))]` nearest neighbors


